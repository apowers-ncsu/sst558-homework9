[
  {
    "objectID": "homework9.html",
    "href": "homework9.html",
    "title": "Homework 9",
    "section": "",
    "text": "The first half of this document is my homework 8 content. We will take the MLR model chosen from HW8, create additional models, and compare to find the best one.\n\n\nThis document demonstrates use of the principles and steps to make models in R:\n\nread data\ncheck the data\nsplit the data\nfit models\napply best model\n\n\n\n\n\nThis work relies heavily on tidymodels packages and related items, so we include this and the standard tidyverse code.\n\n\nWarning: package 'tidymodels' was built under R version 4.4.2\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n\n\n✔ broom        1.0.6     ✔ recipes      1.1.0\n✔ dials        1.3.0     ✔ rsample      1.2.1\n✔ dplyr        1.1.4     ✔ tibble       3.2.1\n✔ ggplot2      3.5.1     ✔ tidyr        1.3.1\n✔ infer        1.0.7     ✔ tune         1.2.1\n✔ modeldata    1.4.0     ✔ workflows    1.1.4\n✔ parsnip      1.2.1     ✔ workflowsets 1.1.0\n✔ purrr        1.0.2     ✔ yardstick    1.3.1\n\n\nWarning: package 'dials' was built under R version 4.4.2\n\n\nWarning: package 'infer' was built under R version 4.4.2\n\n\nWarning: package 'modeldata' was built under R version 4.4.2\n\n\nWarning: package 'parsnip' was built under R version 4.4.2\n\n\nWarning: package 'recipes' was built under R version 4.4.2\n\n\nWarning: package 'rsample' was built under R version 4.4.2\n\n\nWarning: package 'tune' was built under R version 4.4.2\n\n\nWarning: package 'workflows' was built under R version 4.4.2\n\n\nWarning: package 'workflowsets' was built under R version 4.4.2\n\n\nWarning: package 'yardstick' was built under R version 4.4.2\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Learn how to get started at https://www.tidymodels.org/start/\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ lubridate 1.9.3     ✔ stringr   1.5.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::col_factor() masks scales::col_factor()\n✖ purrr::discard()    masks scales::discard()\n✖ dplyr::filter()     masks stats::filter()\n✖ stringr::fixed()    masks recipes::fixed()\n✖ dplyr::lag()        masks stats::lag()\n✖ readr::spec()       masks yardstick::spec()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nWarning: package 'corrr' was built under R version 4.4.2\n\n\nWarning: package 'glmnet' was built under R version 4.4.2\n\n\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nLoaded glmnet 4.1-8\n\n\nWarning: package 'rpart.plot' was built under R version 4.4.2\n\n\nLoading required package: rpart\n\nAttaching package: 'rpart'\n\nThe following object is masked from 'package:dials':\n\n    prune\n\n\nWarning: package 'baguette' was built under R version 4.4.2\n\n\nWarning: package 'ranger' was built under R version 4.4.2\n\n\nWarning: package 'randomForest' was built under R version 4.4.2\n\n\nrandomForest 4.7-1.2\nType rfNews() to see new features/changes/bug fixes.\n\nAttaching package: 'randomForest'\n\nThe following object is masked from 'package:ranger':\n\n    importance\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\n\n\n\nThe data comes from the UCI Machine Learning Repository. This set is about bike sharing rentals. More details available here. The data description describes the following variables:\n\n\n\n\n\n\n\nFIELD\nNOTES\n\n\n\n\nDate\nday/month/year\n\n\nRented Bike count\nCount of bikes rented at each hour\n\n\nHour\nHour of the day\n\n\nTemperature\nTemperature in Celsius\n\n\nHumidity\n%\n\n\nWindspeed\nm/s\n\n\nVisibility\n10m\n\n\nDew point temperature\nCelsius\n\n\nSolar radiation\nMJ/m2\n\n\nRainfall\nmm\n\n\nSnowfall\ncm\n\n\nSeasons\nWinter, Spring, Summer, Autumn\n\n\nHoliday\nHoliday/No holiday\n\n\nFunctional Day\nNoFunc(Non Functional Hours), Fun(Functional hours)\n\n\n\n\n\n\n\n\ndata_url &lt;- \"https://www4.stat.ncsu.edu/~online/datasets/SeoulBikeData.csv\"\ndata_raw &lt;- read_csv(\n  file = data_url, \n  locale = locale(encoding = \"latin1\")\n  )\n\nRows: 8760 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): Date, Seasons, Holiday, Functioning Day\ndbl (10): Rented Bike Count, Hour, Temperature(°C), Humidity(%), Wind speed ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n\nNow, I need to review the data and clean it up, then summarize it.\n\n\n\ndata_raw |&gt; \n  map( ~sum(is.na(.)) )\n\n$Date\n[1] 0\n\n$`Rented Bike Count`\n[1] 0\n\n$Hour\n[1] 0\n\n$`Temperature(°C)`\n[1] 0\n\n$`Humidity(%)`\n[1] 0\n\n$`Wind speed (m/s)`\n[1] 0\n\n$`Visibility (10m)`\n[1] 0\n\n$`Dew point temperature(°C)`\n[1] 0\n\n$`Solar Radiation (MJ/m2)`\n[1] 0\n\n$`Rainfall(mm)`\n[1] 0\n\n$`Snowfall (cm)`\n[1] 0\n\n$Seasons\n[1] 0\n\n$Holiday\n[1] 0\n\n$`Functioning Day`\n[1] 0\n\n\nLooks ok, no missing values (NA).\n\n\n\nDo the column types look accurate?\n\nstr(data_raw)\n\nspc_tbl_ [8,760 × 14] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Date                     : chr [1:8760] \"01/12/2017\" \"01/12/2017\" \"01/12/2017\" \"01/12/2017\" ...\n $ Rented Bike Count        : num [1:8760] 254 204 173 107 78 100 181 460 930 490 ...\n $ Hour                     : num [1:8760] 0 1 2 3 4 5 6 7 8 9 ...\n $ Temperature(°C)          : num [1:8760] -5.2 -5.5 -6 -6.2 -6 -6.4 -6.6 -7.4 -7.6 -6.5 ...\n $ Humidity(%)              : num [1:8760] 37 38 39 40 36 37 35 38 37 27 ...\n $ Wind speed (m/s)         : num [1:8760] 2.2 0.8 1 0.9 2.3 1.5 1.3 0.9 1.1 0.5 ...\n $ Visibility (10m)         : num [1:8760] 2000 2000 2000 2000 2000 ...\n $ Dew point temperature(°C): num [1:8760] -17.6 -17.6 -17.7 -17.6 -18.6 -18.7 -19.5 -19.3 -19.8 -22.4 ...\n $ Solar Radiation (MJ/m2)  : num [1:8760] 0 0 0 0 0 0 0 0 0.01 0.23 ...\n $ Rainfall(mm)             : num [1:8760] 0 0 0 0 0 0 0 0 0 0 ...\n $ Snowfall (cm)            : num [1:8760] 0 0 0 0 0 0 0 0 0 0 ...\n $ Seasons                  : chr [1:8760] \"Winter\" \"Winter\" \"Winter\" \"Winter\" ...\n $ Holiday                  : chr [1:8760] \"No Holiday\" \"No Holiday\" \"No Holiday\" \"No Holiday\" ...\n $ Functioning Day          : chr [1:8760] \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Date = col_character(),\n  ..   `Rented Bike Count` = col_double(),\n  ..   Hour = col_double(),\n  ..   `Temperature(°C)` = col_double(),\n  ..   `Humidity(%)` = col_double(),\n  ..   `Wind speed (m/s)` = col_double(),\n  ..   `Visibility (10m)` = col_double(),\n  ..   `Dew point temperature(°C)` = col_double(),\n  ..   `Solar Radiation (MJ/m2)` = col_double(),\n  ..   `Rainfall(mm)` = col_double(),\n  ..   `Snowfall (cm)` = col_double(),\n  ..   Seasons = col_character(),\n  ..   Holiday = col_character(),\n  ..   `Functioning Day` = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nObservations and updates required:\n\nSwitch type to Date:\n\nDate\n\nSwitch type to Integer:\n\nRented Bike Count\nHour\n\nSwitch character lists to Factor:\n\nSeasons\nHoliday\nFunctioning Day\n\nRename to remove spaces across most fields\n\nDo the numerical summaries look reasonable?\n\ndata_raw |&gt;\n  select(where(is.numeric)) |&gt;\n  psych::describe() |&gt;\n  select(\n    min,\n    max,\n    range,\n    median,\n    sd\n    )\n\n                            min     max   range  median     sd\nRented Bike Count           0.0 3556.00 3556.00  504.50 645.00\nHour                        0.0   23.00   23.00   11.50   6.92\nTemperature(°C)           -17.8   39.40   57.20   13.70  11.94\nHumidity(%)                 0.0   98.00   98.00   57.00  20.36\nWind speed (m/s)            0.0    7.40    7.40    1.50   1.04\nVisibility (10m)           27.0 2000.00 1973.00 1698.00 608.30\nDew point temperature(°C) -30.6   27.20   57.80    5.10  13.06\nSolar Radiation (MJ/m2)     0.0    3.52    3.52    0.01   0.87\nRainfall(mm)                0.0   35.00   35.00    0.00   1.13\nSnowfall (cm)               0.0    8.80    8.80    0.00   0.44\n\n\nNothing looks unreasonable in the numeric variable spread.\nDo the categorical variable values look reasonable?\n\ndata_raw |&gt;\n  select(where(is_character),-Date) |&gt;\n  map(unique)\n\n$Seasons\n[1] \"Winter\" \"Spring\" \"Summer\" \"Autumn\"\n\n$Holiday\n[1] \"No Holiday\" \"Holiday\"   \n\n$`Functioning Day`\n[1] \"Yes\" \"No\" \n\n\nUnique categorical values look fine as well.\n\n\n\nNow, let’s fix the Date field format.\n\ndata_raw &lt;- data_raw |&gt;\n  mutate(Date = as_date(Date,format=\"%d/%m/%Y\"))\nstr(data_raw$Date)\n\n Date[1:8760], format: \"2017-12-01\" \"2017-12-01\" \"2017-12-01\" \"2017-12-01\" \"2017-12-01\" ...\n\n\n\n\n\nNext, turn character fields into factors.\n\ndata_raw &lt;- data_raw |&gt;\n  mutate(\n    Seasons = as_factor(Seasons),\n    Holiday = as_factor(Holiday),\n    `Functioning Day` = as_factor(`Functioning Day`)\n    )\nstr(select(data_raw,where(is.factor)))\n\ntibble [8,760 × 3] (S3: tbl_df/tbl/data.frame)\n $ Seasons        : Factor w/ 4 levels \"Winter\",\"Spring\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ Holiday        : Factor w/ 2 levels \"No Holiday\",\"Holiday\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Functioning Day: Factor w/ 2 levels \"Yes\",\"No\": 1 1 1 1 1 1 1 1 1 1 ...\n\n\nAlso, here, I will change the previously-noted fields into integers.\n\ndata_raw &lt;- data_raw |&gt;\n  mutate(\n    `Rented Bike Count` = as.integer(`Rented Bike Count`),\n    Hour = as.integer(Hour)\n    )\nstr(select(data_raw,where(is_integer)))\n\ntibble [8,760 × 5] (S3: tbl_df/tbl/data.frame)\n $ Rented Bike Count: int [1:8760] 254 204 173 107 78 100 181 460 930 490 ...\n $ Hour             : int [1:8760] 0 1 2 3 4 5 6 7 8 9 ...\n $ Seasons          : Factor w/ 4 levels \"Winter\",\"Spring\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ Holiday          : Factor w/ 2 levels \"No Holiday\",\"Holiday\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Functioning Day  : Factor w/ 2 levels \"Yes\",\"No\": 1 1 1 1 1 1 1 1 1 1 ...\n\n\n\n\n\nAnd lastly, cleaning up the names for easier work without encoding.\n\ndata_raw &lt;- data_raw |&gt;\n  mutate(\n    BikeCount = `Rented Bike Count`,\n    Temperature = `Temperature(°C)`,\n    Humidity = `Humidity(%)`,\n    WindSpeed = `Wind speed (m/s)`,\n    Visibility = `Visibility (10m)`,\n    DewPoint = `Dew point temperature(°C)`,\n    SolarRadiation = `Solar Radiation (MJ/m2)`,\n    Rainfall = `Rainfall(mm)`,\n    Snowfall = `Snowfall (cm)`,\n    FunctioningDay = `Functioning Day`,         \n    .keep='unused'\n    )\nstr(data_raw)\n\ntibble [8,760 × 14] (S3: tbl_df/tbl/data.frame)\n $ Date          : Date[1:8760], format: \"2017-12-01\" \"2017-12-01\" ...\n $ Hour          : int [1:8760] 0 1 2 3 4 5 6 7 8 9 ...\n $ Seasons       : Factor w/ 4 levels \"Winter\",\"Spring\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ Holiday       : Factor w/ 2 levels \"No Holiday\",\"Holiday\": 1 1 1 1 1 1 1 1 1 1 ...\n $ BikeCount     : int [1:8760] 254 204 173 107 78 100 181 460 930 490 ...\n $ Temperature   : num [1:8760] -5.2 -5.5 -6 -6.2 -6 -6.4 -6.6 -7.4 -7.6 -6.5 ...\n $ Humidity      : num [1:8760] 37 38 39 40 36 37 35 38 37 27 ...\n $ WindSpeed     : num [1:8760] 2.2 0.8 1 0.9 2.3 1.5 1.3 0.9 1.1 0.5 ...\n $ Visibility    : num [1:8760] 2000 2000 2000 2000 2000 ...\n $ DewPoint      : num [1:8760] -17.6 -17.6 -17.7 -17.6 -18.6 -18.7 -19.5 -19.3 -19.8 -22.4 ...\n $ SolarRadiation: num [1:8760] 0 0 0 0 0 0 0 0 0.01 0.23 ...\n $ Rainfall      : num [1:8760] 0 0 0 0 0 0 0 0 0 0 ...\n $ Snowfall      : num [1:8760] 0 0 0 0 0 0 0 0 0 0 ...\n $ FunctioningDay: Factor w/ 2 levels \"Yes\",\"No\": 1 1 1 1 1 1 1 1 1 1 ...\n\n\n\n\n\nNow, to display some summaries of the tidied data. Numeric summaries and then categorical contingency tables.\n\ndata_raw |&gt;\n  select(where(is.numeric)) |&gt;\n  psych::describe() |&gt;\n  select(\n    min,\n    max,\n    range,\n    median,\n    sd\n    )\n\n                 min     max   range  median     sd\nHour             0.0   23.00   23.00   11.50   6.92\nBikeCount        0.0 3556.00 3556.00  504.50 645.00\nTemperature    -17.8   39.40   57.20   13.70  11.94\nHumidity         0.0   98.00   98.00   57.00  20.36\nWindSpeed        0.0    7.40    7.40    1.50   1.04\nVisibility      27.0 2000.00 1973.00 1698.00 608.30\nDewPoint       -30.6   27.20   57.80    5.10  13.06\nSolarRadiation   0.0    3.52    3.52    0.01   0.87\nRainfall         0.0   35.00   35.00    0.00   1.13\nSnowfall         0.0    8.80    8.80    0.00   0.44\n\n\nNothing stands out here, as noted earlier. Now, to contingency tables for categorical variables.\n\ndata_raw |&gt;\n  group_by(Seasons) |&gt;\n  summarize(n())\n\n# A tibble: 4 × 2\n  Seasons `n()`\n  &lt;fct&gt;   &lt;int&gt;\n1 Winter   2160\n2 Spring   2208\n3 Summer   2208\n4 Autumn   2184\n\n\n\ndata_raw |&gt;\n  group_by(Holiday) |&gt;\n  summarize(n())\n\n# A tibble: 2 × 2\n  Holiday    `n()`\n  &lt;fct&gt;      &lt;int&gt;\n1 No Holiday  8328\n2 Holiday      432\n\n\n\ndata_raw |&gt;\n  group_by(FunctioningDay) |&gt;\n  summarize(n())\n\n# A tibble: 2 × 2\n  FunctioningDay `n()`\n  &lt;fct&gt;          &lt;int&gt;\n1 Yes             8465\n2 No               295\n\n\n\ndata_raw |&gt;\n  group_by(FunctioningDay,Seasons) |&gt;\n  summarize(n())\n\n`summarise()` has grouped output by 'FunctioningDay'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 6 × 3\n# Groups:   FunctioningDay [2]\n  FunctioningDay Seasons `n()`\n  &lt;fct&gt;          &lt;fct&gt;   &lt;int&gt;\n1 Yes            Winter   2160\n2 Yes            Spring   2160\n3 Yes            Summer   2208\n4 Yes            Autumn   1937\n5 No             Spring     48\n6 No             Autumn    247\n\n\nI don’t understand truly what the FunctioningDay field means. The notes say it is a target / response variable, but exactly how to interpret that is unclear to me. I’ll check grouping by this field.\n\ndata_raw |&gt;\n  group_by(FunctioningDay) |&gt;\n  summarize(\n    Min=min(BikeCount),\n    Max=max(BikeCount),\n    Avg=mean(BikeCount)\n    )\n\n# A tibble: 2 × 4\n  FunctioningDay   Min   Max   Avg\n  &lt;fct&gt;          &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n1 Yes                2  3556  729.\n2 No                 0     0    0 \n\n\nOh, it is simply an indicator of when bikes were available. I presume we do not want to study the days when bikes did not allow usage, so now we will subset to remove those days (FunctioningDay = No).\n\ndata_raw &lt;- data_raw |&gt;\n  filter(FunctioningDay == 'Yes')\n\n\n\n\nNow for simplicity, we adjust our data to summarize across hours so that each day has only one observation associated with it.\n\ndata &lt;- data_raw |&gt;\n  group_by(Date,\n           Seasons,\n           Holiday\n           ) |&gt;\n  summarize(\n    BikeCountSum = sum(BikeCount),\n    RainfallSum = sum(Rainfall),\n    SnowfallSum = sum(Snowfall),\n    TemperatureAvg = mean(Temperature),\n    HumidityAvg = mean(Humidity),\n    WindSpeedAvg = mean(WindSpeed),\n    VisibilityAvg = mean(Visibility),\n    DewPointAvg = mean(DewPoint),\n    SolarRadiationAvg = mean(SolarRadiation)\n    ) |&gt;\n  select(\n    Date,\n    Seasons,\n    Holiday,\n    ends_with(\"Sum\"),\n    ends_with(\"Avg\")\n  )\n\n`summarise()` has grouped output by 'Date', 'Seasons'. You can override using\nthe `.groups` argument.\n\nhead(data)\n\n# A tibble: 6 × 12\n# Groups:   Date, Seasons [6]\n  Date       Seasons Holiday BikeCountSum RainfallSum SnowfallSum TemperatureAvg\n  &lt;date&gt;     &lt;fct&gt;   &lt;fct&gt;          &lt;int&gt;       &lt;dbl&gt;       &lt;dbl&gt;          &lt;dbl&gt;\n1 2017-12-01 Winter  No Hol…         9539         0           0          -2.45  \n2 2017-12-02 Winter  No Hol…         8523         0           0           1.32  \n3 2017-12-03 Winter  No Hol…         7222         4           0           4.88  \n4 2017-12-04 Winter  No Hol…         8729         0.1         0          -0.304 \n5 2017-12-05 Winter  No Hol…         8307         0           0          -4.46  \n6 2017-12-06 Winter  No Hol…         6669         1.3         8.6         0.0458\n# ℹ 5 more variables: HumidityAvg &lt;dbl&gt;, WindSpeedAvg &lt;dbl&gt;,\n#   VisibilityAvg &lt;dbl&gt;, DewPointAvg &lt;dbl&gt;, SolarRadiationAvg &lt;dbl&gt;\n\n\n\n\n\nNow, to restate summaries of the updated dataset.\n\ndata |&gt;\n  select(where(is.numeric)) |&gt;\n  psych::describe() |&gt;\n  select(\n    min,\n    max,\n    range,\n    median,\n    sd\n    )\n\nAdding missing grouping variables: `Date`, `Seasons`\n\n\nWarning in FUN(newX[, i], ...): no non-missing arguments to min; returning Inf\n\n\nWarning in FUN(newX[, i], ...): no non-missing arguments to max; returning -Inf\n\n\n                     min      max    range   median      sd\nDate                 Inf     -Inf     -Inf       NA      NA\nSeasons*            1.00     4.00     3.00     2.00    1.11\nBikeCountSum      977.00 36149.00 35172.00 18563.00 9937.16\nRainfallSum         0.00    95.50    95.50     0.00   11.79\nSnowfallSum         0.00    78.70    78.70     0.00    8.80\nTemperatureAvg    -14.74    33.74    48.48    13.74   11.72\nHumidityAvg        22.25    95.88    73.62    57.17   14.87\nWindSpeedAvg        0.66     4.00     3.34     1.66    0.60\nVisibilityAvg     214.29  2000.00  1785.71  1557.75  491.16\nDewPointAvg       -27.75    25.04    52.79     4.61   12.99\nSolarRadiationAvg   0.03     1.22     1.19     0.56    0.32\n\n\nLet’s visualize this information a few ways - with box and whiskers as well as scatterplots.\n\ng &lt;- data |&gt;\n  ggplot()\ng + \n  geom_boxplot(\n    aes(\n      x=Seasons,\n      y=BikeCountSum,\n      color=Holiday\n    )\n  ) + \n  labs(\n    title=\"Bike Counts per Season by Holiday\"\n    )\n\n\n\n\n\n\n\n\nOn holidays, across all seasons, fewer bikes are used. However, the variation in range of max and min bikes used is much smaller on holidays. So, as a light interpretation notwithstanding the much smaller sample size of Holiday data, we might assess that holidays do garner a tight range of activity, consistently.\n\ng + \n  geom_point(\n    aes(\n      x=TemperatureAvg,\n      y=BikeCountSum\n    )\n  ) + \n  labs(\n    title=\"Bike Counts vs Temperature\"\n    ) +\n  facet_grid(~Seasons)\n\n\n\n\n\n\n\n\nThe shapes here are interesting. In Winter, no matter the temperature, few bikes are used. In the spring, where it can be a bit cool to a bit warm, the number of bikes used quickly grows. In the summer, in high temperatures consistently, if temperature raises slightly, bike rentals decrease rapidly. Autumn is comparable to Spring in shape and range.\nLastly, we display correlations for all numeric variables.\n\ndata |&gt;\n  select(where(is.numeric)) |&gt;\n  correlate() |&gt;\n  shave() |&gt;\n  rplot()\n\nAdding missing grouping variables: `Date`, `Seasons`\nNon-numeric variables removed from input: `Date`, and `Seasons`\nCorrelation computed with • Method: 'pearson' • Missing treated using:\n'pairwise.complete.obs'\n\n\n\n\n\n\n\n\n\nThis package corrr has cool features, including this color-coded display of all correlations between numeric variables. Immediately, we can see the strongest relationships with Bike Counts are the Temperature, Dew Point, and Solar Radiation. It’s likely those are interrelated and tell the same story (evidenced by the strong correlation between Temperature and Dew Point shown in the chart, elsewhere). The strongest negative correlation between non-result variables is that of Humidity and Visibility. I don’t normally think of humidity impacting visibility, so that’s interesting; is it because of pollution or am I simply unaware that wet air does impede visibility, perhaps at longer distances?\n\n\n\n\nTo analyze this data, which is small, we will split into training and test and then use 10-fold CV. In the split, we will use the strata argument to ensure a fair sample across the seasons variable.\n\ndata_split &lt;- initial_split(data, prop = 0.75, strata = Seasons)\ndata_train &lt;- training(data_split)\ndata_test &lt;- testing(data_split)\ndata_train_10Fold &lt;- vfold_cv(data_train, 10)\n\n\n\n\n\n\nFirst recipe, ignore Date and instead work with weekday/weekend factor. Then standardize numeric variables to make comparable scales. Create dummy variables for seasons, holiday, and the day type.\n\nrecipe1 &lt;- recipe(BikeCountSum ~ ., data = data_train) |&gt;\n  \n  #Date into weekend/weekday\n  step_date(Date) |&gt;\n  step_mutate(\n    Weekday_Weekend = factor(if_else(\n      (Date_dow == \"Sat\") | (Date_dow == \"Sun\"),\n      \"Weekend\",\n      \"Weekday\")\n      )\n    ) |&gt;\n  \n  #remove excess original Date fields\n  step_rm(c(Date,\n            Date_dow,\n            Date_month,\n            Date_year)\n          ) |&gt;\n  \n  #normalize numerics\n  step_normalize(\n    all_numeric(),\n    -all_outcomes()\n    ) |&gt;\n  \n  #dummy vars for categorical items\n  step_dummy(c(Seasons,\n               Holiday,\n               Weekday_Weekend)\n             ) |&gt;\n  \n  #clean up names\n  step_rename(\n    isHoliday = Holiday_Holiday,\n    isWeekend = Weekday_Weekend_Weekend,\n    isSummerSeason = Seasons_Summer,\n    isSpringSeason = Seasons_Spring,\n    isAutumnSeason = Seasons_Autumn\n  )\n    \n    \n   # ) |&gt;  prep(training=data_train) |&gt;\n #bake(data_train)\n#testing |&gt; summary()\n\n\n\n\nFor this recipe, we start with Recipe 1 and add interaction terms between:\n\nseasons and holiday\nseasons and temp\ntemp and rainfall\n\n\nrecipe2 &lt;- recipe1 |&gt;\n  step_interact(terms = ~\n                  ends_with(\"Season\") *\n                  ends_with(\"Holiday\") \n                ) |&gt;\n  step_interact(terms = ~\n                  ends_with(\"Season\") *\n                  TemperatureAvg\n                ) |&gt;\n  step_interact(terms = ~\n                  TemperatureAvg *\n                  RainfallSum\n                ) \n\n\n\n\nFor the third recipe, start from Recipe 2 and add quadratic terms for each numeric predictor. Since our dummy variables are technically numeric now, I’m excluding them by avoiding all those beginning with is (like isSpring, etc.).\n\nrecipe3 &lt;- recipe2 |&gt;\n  step_poly(\n    all_numeric_predictors(),\n    -starts_with(\"is\"),\n    degree=2\n    )\n\n\n\n\nWe will fit the models using linear lm engine and use 10-fold CV to calculate error.\nFirst, define the model engine.\n\ndata_model &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\n\nNext, define workflows for each recipe.\n\ndata_workflow1 &lt;- workflow() |&gt;\n  add_recipe(recipe1) |&gt;\n  add_model(data_model)\n\ndata_workflow2 &lt;- workflow() |&gt;\n  add_recipe(recipe2) |&gt;\n  add_model(data_model)\n\ndata_workflow3 &lt;- workflow() |&gt;\n  add_recipe(recipe3) |&gt;\n  add_model(data_model)\n\nNow, define and run the 10-fold CV for each. Out of curiosity, I am going to compare to a non-CV run as well.\n\n#non-CV for simple recipe 1\ndata_fit_nonCV &lt;- data_workflow1 |&gt;\n  fit(data_train)\n\n#data_fit_nonCV |&gt;\n# tidy()\n\n#10fold CV for each recipe\nrecipe1_10Fold_metrics &lt;- data_workflow1 |&gt;\n  fit_resamples(data_train_10Fold) |&gt;\n  collect_metrics()\n\nrecipe2_10Fold_metrics &lt;- data_workflow2 |&gt;\n  fit_resamples(data_train_10Fold) |&gt;\n  collect_metrics()\n\n→ A | warning: prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x2\nThere were issues with some computations   A: x2\n\n\n\n\nrecipe3_10Fold_metrics &lt;- data_workflow3 |&gt;\n  fit_resamples(data_train_10Fold) |&gt;\n  collect_metrics()\n\n→ A | warning: prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\n\n\nThere were issues with some computations   A: x2\nThere were issues with some computations   A: x2\n\n\n\n\nrbind(\n  recipe1_10Fold_metrics,\n  recipe2_10Fold_metrics,\n  recipe3_10Fold_metrics\n)\n\n# A tibble: 6 × 6\n  .metric .estimator     mean     n  std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   4068.       10 199.     Preprocessor1_Model1\n2 rsq     standard      0.821    10   0.0231 Preprocessor1_Model1\n3 rmse    standard   3133.       10 227.     Preprocessor1_Model1\n4 rsq     standard      0.891    10   0.0176 Preprocessor1_Model1\n5 rmse    standard   2899.       10 232.     Preprocessor1_Model1\n6 rsq     standard      0.906    10   0.0176 Preprocessor1_Model1\n\n\nThe best model of the three looks like the third recipe, with interaction terms and quadratic terms.\n\n\n\n\nNow, let’s fit it to the entire training dataset and compute RMSE.\n\nbest_fit &lt;- data_workflow3 |&gt;\n  last_fit(data_split)\nbest_fit |&gt; collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    3569.    Preprocessor1_Model1\n2 rsq     standard       0.895 Preprocessor1_Model1\n\n\nHere is the coefficient table for our model, arranged by p-values to highlight the most predictive parameters.\n\nextract_fit_parsnip(best_fit) |&gt; tidy() |&gt; arrange(p.value)\n\n# A tibble: 30 × 5\n   term                                estimate std.error statistic  p.value\n   &lt;chr&gt;                                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 isSummerSeason                        26491.     2012.     13.2  5.75e-30\n 2 SolarRadiationAvg_poly_1              49177.     5662.      8.68 6.74e-16\n 3 isAutumnSeason                        12903.     2114.      6.10 4.29e- 9\n 4 isWeekend                             -2308.      398.     -5.80 2.18e- 8\n 5 isSpringSeason_x_TemperatureAvg       11376.     2363.      4.82 2.65e- 6\n 6 RainfallSum_poly_1                   -51088.    12393.     -4.12 5.21e- 5\n 7 isSpringSeason                         7863.     2069.      3.80 1.84e- 4\n 8 (Intercept)                            8535.     2522.      3.38 8.37e- 4\n 9 TemperatureAvg_x_RainfallSum_poly_2   28985.     9360.      3.10 2.20e- 3\n10 isAutumnSeason_x_TemperatureAvg        7050.     2665.      2.65 8.71e- 3\n# ℹ 20 more rows\n\n\nSo, recalling what we are doing here - predicting bike rental volume - it is interesting to note the predictors most likely to relate to bike rental volumes. I think that’s what the lowest p-values represent here, the likelihood that this was a random relationship (slope of zero) with the outcome.\n\nif we are in summer, we are likely to see more rentals\nsolar radiation increases with rentals, too (related to summer)\non the weekend, we are less likely to see rentals? That surprises me, so I checked my setup to be sure.\nif raining, less bikes; this makes sense.\n\n\n\n\n\n\nNow, we will proceed to create the following models. I can reuse data objects and recipes from the prior effort (homework 8)!\n\na (tuned) LASSO model\na (tuned) Regression Tree model\na (tuned) Bagged Tree model\na (tuned) Random Forest model\n\nI will fit and tune each on the training set, taking the best from each family, fitting on the entire training set, and then comparing the loss functions of each family on the test set. Metrics will be RMSE and MAE. Will also display some data on each of the model types and finally fit the winner to the full dataset.\n\n\nWith my MLR recipes above, the best performing result included interaction terms and quadratic terms - presuming I did that correctly, of course. Since I have no other reason to choose or exclude terms, I’ll work from that recipe again, in the LASSO framework.\nAlso, note that I’m renaming things for some clarity with competing models. My MLR models are labeled as workflow_final_mlr for the best of the MLR family of models and fit_mlr for the version of this that has trained on the entire training dataset.\n\n#renaming my mlr workflow and model and fit\nworkflow_final_mlr &lt;- data_workflow3\nmodel_mlr &lt;- data_model\nfit_mlr &lt;- workflow_final_mlr |&gt; fit(data_train)\nrecipe_mlr &lt;- recipe3\n\n#setting lasso model and workflow\nmodel_lasso &lt;- linear_reg(penalty = tune(), mixture = 1) |&gt;\n  set_engine(\"glmnet\")\n\nworkflow_lasso &lt;- workflow() |&gt;\n  add_recipe(recipe_mlr) |&gt;\n  add_model(model_lasso)\n\nNow I configure the grid for tuning to find the optimal alpha value. I don’t know what alphas to try so I’m using grid_regular().\n\n#A warning will occur for one value of the tuning parameter, safe to ignore\ngrid_lasso &lt;- workflow_lasso |&gt;\n  tune_grid(\n    resamples = data_train_10Fold,\n    grid = grid_regular(penalty(), levels = 200)*25\n    ) \n\ngrid_lasso |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\")\n\n# A tibble: 200 × 7\n         penalty .metric .estimator  mean     n std_err .config               \n           &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n 1 0.0000000025  rmse    standard   2885.    10    229. Preprocessor1_Model001\n 2 0.00000000281 rmse    standard   2885.    10    229. Preprocessor1_Model002\n 3 0.00000000315 rmse    standard   2885.    10    229. Preprocessor1_Model003\n 4 0.00000000354 rmse    standard   2885.    10    229. Preprocessor1_Model004\n 5 0.00000000397 rmse    standard   2885.    10    229. Preprocessor1_Model005\n 6 0.00000000446 rmse    standard   2885.    10    229. Preprocessor1_Model006\n 7 0.00000000501 rmse    standard   2885.    10    229. Preprocessor1_Model007\n 8 0.00000000562 rmse    standard   2885.    10    229. Preprocessor1_Model008\n 9 0.00000000631 rmse    standard   2885.    10    229. Preprocessor1_Model009\n10 0.00000000708 rmse    standard   2885.    10    229. Preprocessor1_Model010\n# ℹ 190 more rows\n\n\nI had to play with the grid here. Penalties (alphas) between 0 and 1 did not differ. Once I scaled them up a bit, I found some improvements around alpha=10. This is clearly visible with a plot.\n\ngrid_lasso |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  ggplot(aes(penalty, mean, color = .metric)) +\n  geom_line()\n\n\n\n\n\n\n\n\nNow, choosing the optimal alpha is easy with tidymodels.\n\ntune_lasso &lt;- grid_lasso |&gt;\n  select_best(metric = \"rmse\")\ntune_lasso\n\n# A tibble: 1 × 2\n  penalty .config               \n    &lt;dbl&gt; &lt;chr&gt;                 \n1    19.8 Preprocessor1_Model198\n\n\nI think the variations across runs are pretty interesting. Based on sample variants each time, I see the ideal penalty land somewhere between 5 and 20, according to the curves generated for each sample. It makes me think I have errors but then I recall and realize that it is normal variance. I suspect too that the model is only lightly influenced by penalty variations in this range, so a model with alpha of 5 is generating a similar prediction to the same with alpha of 20.\nFinishing this workflow, now training with the best alpha on the full training set, to complete the LASSO effort. I’m storing the model in workflow_final_lasso and the fit model in fit_lasso.\nThe coefficient tables for this model:\n\nworkflow_final_lasso &lt;- workflow_lasso |&gt;\n  finalize_workflow(tune_lasso)\nfit_lasso &lt;- workflow_final_lasso |&gt;\n  fit(data_train)\ntidy(fit_lasso)\n\n# A tibble: 30 × 3\n   term                            estimate penalty\n   &lt;chr&gt;                              &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)                       10039.    19.8\n 2 isSpringSeason                     6501.    19.8\n 3 isSummerSeason                    24670.    19.8\n 4 isAutumnSeason                    11475.    19.8\n 5 isHoliday                         -2830.    19.8\n 6 isWeekend                         -2352.    19.8\n 7 isSpringSeason_x_isHoliday        -3498.    19.8\n 8 isSummerSeason_x_isHoliday        -1283.    19.8\n 9 isAutumnSeason_x_isHoliday        -4763.    19.8\n10 isSpringSeason_x_TemperatureAvg   10371.    19.8\n# ℹ 20 more rows\n\n\n\n\n\nNext, a tuned regression tree model. I’m using the original recipe, less the interaction terms that are irrelevant for this model family. As I barely understand the model details, I’m going to tune all 3 parameters.\n\nrecipe_regtree &lt;- recipe_mlr |&gt;\n  step_rm(contains(\"_x_\"))\n\nmodel_regtree &lt;- \n  decision_tree(\n    tree_depth = tune(),\n    #min_n = 20,\n    min_n = tune(),\n    cost_complexity = tune()\n    ) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"regression\")\n\nworkflow_regtree &lt;- workflow() |&gt;\n  add_recipe(recipe_regtree) |&gt;\n  add_model(model_regtree)\n\nNext, setting up the tuning grid and using CV to find the options and metrics. I’m first trying to let it pick the grid itself via the dials package.\n\ntemp &lt;- workflow_regtree |&gt; \n  tune_grid(\n    resamples = data_train_10Fold\n    )\n\ntemp |&gt; \n  collect_metrics()\n\n# A tibble: 20 × 9\n   cost_complexity tree_depth min_n .metric .estimator     mean     n  std_err\n             &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt;\n 1        4.21e- 4          9    22 rmse    standard   3769.       10 331.    \n 2        4.21e- 4          9    22 rsq     standard      0.837    10   0.0322\n 3        3.05e- 9         14    34 rmse    standard   3952.       10 286.    \n 4        3.05e- 9         14    34 rsq     standard      0.823    10   0.0292\n 5        7.47e- 9         12     2 rmse    standard   3859.       10 228.    \n 6        7.47e- 9         12     2 rsq     standard      0.847    10   0.0185\n 7        8.82e- 5          5    28 rmse    standard   3971.       10 282.    \n 8        8.82e- 5          5    28 rsq     standard      0.823    10   0.0292\n 9        5.35e- 7          3     9 rmse    standard   4127.       10 284.    \n10        5.35e- 7          3     9 rsq     standard      0.815    10   0.0294\n11        4.39e- 2         12    31 rmse    standard   4726.       10 281.    \n12        4.39e- 2         12    31 rsq     standard      0.756    10   0.0315\n13        1.84e-10          5    12 rmse    standard   3850.       10 227.    \n14        1.84e-10          5    12 rsq     standard      0.838    10   0.0267\n15        1.73e- 5          1    14 rmse    standard   6362.       10 266.    \n16        1.73e- 5          1    14 rsq     standard      0.567    10   0.0377\n17        5.99e- 3          7    40 rmse    standard   4033.       10 305.    \n18        5.99e- 3          7    40 rsq     standard      0.818    10   0.0303\n19        1.18e- 7          8    18 rmse    standard   3561.       10 280.    \n20        1.18e- 7          8    18 rsq     standard      0.859    10   0.0286\n# ℹ 1 more variable: .config &lt;chr&gt;\n\n\nThat doesn’t look like enough variety to me, though I lack experience to qualify that point of view. Nonetheless, I’m going to try a larger grid and check whether the results improve.\n\ngrid_regtree &lt;- grid_regular(\n  cost_complexity(),\n  #tree_depth(range = c(3,8)),\n  tree_depth(),\n  min_n(),\n  levels = c(5, 5, 5))\n\ntemp &lt;- workflow_regtree |&gt; \n  tune_grid(\n    resamples = data_train_10Fold,\n    grid=grid_regtree\n    )\n\ntemp |&gt; \n  collect_metrics() |&gt;\n  filter(.metric==\"rmse\") |&gt;\n  arrange(mean)\n\n# A tibble: 125 × 9\n   cost_complexity tree_depth min_n .metric .estimator  mean     n std_err\n             &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n 1    0.000562              8    11 rmse    standard   3651.    10    258.\n 2    0.000562             11    11 rmse    standard   3659.    10    261.\n 3    0.000562             15    11 rmse    standard   3659.    10    261.\n 4    0.0000000001          8    11 rmse    standard   3681.    10    260.\n 5    0.0000000178          8    11 rmse    standard   3681.    10    260.\n 6    0.00000316            8    11 rmse    standard   3681.    10    260.\n 7    0.0000000001         11    11 rmse    standard   3689.    10    263.\n 8    0.0000000178         11    11 rmse    standard   3689.    10    263.\n 9    0.00000316           11    11 rmse    standard   3689.    10    263.\n10    0.0000000001         15    11 rmse    standard   3689.    10    263.\n# ℹ 115 more rows\n# ℹ 1 more variable: .config &lt;chr&gt;\n\n\nSo, that does tend to get me better results. And running this with only 125 instances rather than 1000 seems quick enough to be reasonable. Just saving the best tuning results in proper variables now:\n\ntune_regtree &lt;- temp |&gt; select_best(metric = \"rmse\")\nworkflow_final_regtree &lt;- \n  workflow_regtree |&gt;\n  finalize_workflow(tune_regtree)\n\nFinishing the fit on full training, with best tuning parameters. Model stored in workflow_final_regtree and model fit to training data in fit_regtree.\n\nfit_regtree &lt;- workflow_final_regtree |&gt;\n  fit(data_train)\n\nPlotting this monster, visually, if possible…\n\nfit_regtree |&gt; \n  extract_fit_engine() |&gt; \n  rpart.plot(roundint=FALSE)\n\n\n\n\n\n\n\n\nSo, that is super unreadable - as the lecture notes said, exchanging accuracy for interpretability. I could limit the levels parameters but that would be an arbitrary change only for readability, which I want to avoid. Temperature, solar radiation are key predictors, but then things vary by other measures of season/temperature. I’m surprised to see such correlated items appear; I might have guessed that this model would emphasize only one of a correlated set of variables - perhaps the strongest temperature / climate / season predictor, then a shift to something unrelated that was the next stronger predictor.\n\n\n\nMoving on to bagged tree, tuning similarly, etc. I’m reusing my recipe from Regression Tree, which again removes the Interaction terms. I’m going to tune it all again, though I wonder if I might simply reuse the tuning parameters from my Regression Tree effort. I will use a smaller grid for tuning.\nAlso, while I don’t need to use my old CV folds here and could use OOB, I don’t know how - and a quick search couldn’t clear it up. So, sticking with the samples in the lectures!\n\nmodel_bagtree &lt;- \n  bag_tree(\n    tree_depth = tune(), \n    min_n = tune(), \n    cost_complexity = tune()\n    ) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"regression\")\n\nworkflow_bagtree &lt;- workflow() |&gt;\n  add_recipe(recipe_regtree) |&gt;\n  add_model(model_bagtree)\n\ngrid_bagtree &lt;- grid_regular(\n  cost_complexity(),\n  tree_depth(),\n  min_n(),\n  levels = c(3, 3, 3)\n  )\n\ntemp &lt;- workflow_bagtree |&gt; \n  tune_grid(\n    resamples = data_train_10Fold,\n    grid=grid_bagtree,\n    metrics = metric_set(rmse,mae)\n    )\n\ntemp |&gt; \n  collect_metrics() |&gt;\n  #filter(.metric==\"rmse\") |&gt;\n  arrange(mean)\n\n# A tibble: 54 × 9\n   cost_complexity tree_depth min_n .metric .estimator  mean     n std_err\n             &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n 1    0.00000316            8     2 mae     standard   2214.    10    167.\n 2    0.00000316           15     2 mae     standard   2287.    10    162.\n 3    0.0000000001          8     2 mae     standard   2320.    10    185.\n 4    0.0000000001         15     2 mae     standard   2321.    10    151.\n 5    0.00000316            8    21 mae     standard   2588.    10    116.\n 6    0.0000000001          8    21 mae     standard   2597.    10    150.\n 7    0.00000316           15    21 mae     standard   2631.    10    128.\n 8    0.0000000001         15    21 mae     standard   2653.    10    166.\n 9    0.00000316           15    40 mae     standard   2862.    10    115.\n10    0.0000000001          8    40 mae     standard   2872.    10    146.\n# ℹ 44 more rows\n# ℹ 1 more variable: .config &lt;chr&gt;\n\ntune_bagtree &lt;- temp |&gt; select_best(metric=\"rmse\")\n\nNow, tuning collected, I set the final workflow and train on full training set.\n\nworkflow_final_bagtree &lt;- \n  workflow_bagtree |&gt;\n  finalize_workflow(tune_bagtree)\n\nfit_bagtree &lt;- \n  workflow_final_bagtree |&gt;\n  fit(data_train)\n\nTo visualize this one, I’ll plot each variable by its importance.\n\nextract_fit_engine(fit_bagtree)$imp |&gt;\n mutate(term = factor(term, levels = term)) |&gt;\n ggplot(aes(x = term, y = value)) +\n geom_bar(stat =\"identity\") +\n coord_flip()\n\n\n\n\n\n\n\n\nThe key predictors here are temperature, dew point, solar radiation, and humidity. These are correlated, especially temperature and solar radiation, and humidity with dew point (and temperature too), etc. Lowest strength predictors are holiday, weekend, visibility. Strange because I thought from my MLR earlier that some of those were stronger - or perhaps it is that they are related to bike volumes, yes, but not strong predictors. I hope that is the case, but I fear too that my MLR may have an error in a step.\n\n\n\nNext, tuning a random forest model, finalizing, and charting variables by importance as above. Same recipe as for other trees. Again, tuning everything algorithmically because I don’t have experience to choose any specific values. And I don’t know how to specify use of the out-of-bag observations, etc.\n\nmodel_randomforest &lt;- \n  rand_forest(\n    mtry = tune(),\n    trees = tune(),\n    min_n = tune()\n    ) |&gt;\n  set_engine(\"ranger\",importance=\"impurity\") |&gt;\n  set_mode(\"regression\")\n\nworkflow_randomforest &lt;- \n  workflow() |&gt;\n  add_recipe(recipe_regtree) |&gt;\n  add_model(model_randomforest)\n\ngrid_randomforest &lt;- \n  grid_regular(\n    mtry(range=c(1,length(recipe_regtree$var_info$role==\"predictor\"))-1),\n    trees(),\n    min_n(),\n    levels = c(3, 3, 3)\n    )\n\ntemp &lt;- workflow_randomforest |&gt; \n  tune_grid(\n    resamples = data_train_10Fold,\n    grid=grid_randomforest,\n    metrics = metric_set(rmse)\n    )\n\ntemp |&gt; \n  collect_metrics() |&gt;\n  #filter(.metric==\"rmse\") |&gt;\n  arrange(mean)\n\n# A tibble: 27 × 9\n    mtry trees min_n .metric .estimator  mean     n std_err .config             \n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n 1    11  2000     2 rmse    standard   2980.    10    183. Preprocessor1_Model…\n 2    11  1000     2 rmse    standard   2992.    10    183. Preprocessor1_Model…\n 3     5  1000     2 rmse    standard   3154.    10    136. Preprocessor1_Model…\n 4     5  2000     2 rmse    standard   3177.    10    149. Preprocessor1_Model…\n 5     0  2000     2 rmse    standard   3224.    10    139. Preprocessor1_Model…\n 6     0  1000     2 rmse    standard   3225.    10    139. Preprocessor1_Model…\n 7    11  1000    21 rmse    standard   3233.    10    152. Preprocessor1_Model…\n 8    11  2000    21 rmse    standard   3234.    10    151. Preprocessor1_Model…\n 9     5  1000    21 rmse    standard   3485.    10    130. Preprocessor1_Model…\n10     5  2000    21 rmse    standard   3512.    10    133. Preprocessor1_Model…\n# ℹ 17 more rows\n\ntune_randomforest &lt;- temp |&gt; select_best(metric=\"rmse\")\n\nNow, tuning collected, I set the final workflow and train on full training set. Then visualize the variables by importance.\n\nworkflow_final_randomforest &lt;- \n  workflow_randomforest |&gt;\n  finalize_workflow(tune_randomforest)\n\nfit_randomforest &lt;- \n  workflow_final_randomforest |&gt;\n  fit(data_train)\n\nTo visualize this one, I’ll plot each variable by its importance. This is a bit trickier and I need to realign the values and names a bit for the random forest model.\n\n#first pull out the variable.importance field\nimp_randomforest &lt;- data.frame(\n  row.names = NULL,\n  names=names(extract_fit_engine(fit_randomforest)$variable.importance),\n  values=extract_fit_engine(fit_randomforest)$variable.importance\n  )\n\n#sort for ggplot, which i do not understand\nimp_randomforest$names &lt;- \n  factor(\n    imp_randomforest$names, \n    levels = imp_randomforest$names[order(imp_randomforest$values,decreasing = TRUE)]\n  )\n\nimp_randomforest |&gt;\n  ggplot(\n    aes(\n      x=names,\n      y=values\n    )\n  ) +\n  geom_bar(stat=\"identity\") +\n  coord_flip()\n\n\n\n\n\n\n\n\nThis result aligns well with the bagged tree model; temperature, dew point, solar radiation are all still the top predictors, by a long margin.\n\n\n\n\nNow, I’ll compare the final fits of all 5 models and their loss metrics (rmse, mae) on the test dataset. I wish it were easier to make these into a single, nice tibble; I know it is possible and not hard, but I’m low on time and going to leave it a bit ugly.\n\nworkflow_final_mlr |&gt;\n  last_fit(data_split,\n           metrics = metric_set(rmse,mae)\n           ) |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       3569. Preprocessor1_Model1\n2 mae     standard       2514. Preprocessor1_Model1\n\nworkflow_final_lasso |&gt;\n  last_fit(data_split,\n           metrics = metric_set(rmse,mae)\n           ) |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       3035. Preprocessor1_Model1\n2 mae     standard       2292. Preprocessor1_Model1\n\nworkflow_final_regtree |&gt;\n  last_fit(data_split,\n           metrics = metric_set(rmse,mae)\n           ) |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       3624. Preprocessor1_Model1\n2 mae     standard       2721. Preprocessor1_Model1\n\nworkflow_final_bagtree |&gt;\n  last_fit(data_split,\n           metrics = metric_set(rmse,mae)\n           ) |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       2714. Preprocessor1_Model1\n2 mae     standard       2160. Preprocessor1_Model1\n\nworkflow_final_randomforest |&gt;\n  last_fit(data_split,\n           metrics = metric_set(rmse,mae)\n           ) |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       3015. Preprocessor1_Model1\n2 mae     standard       2387. Preprocessor1_Model1\n\n\nBased on my evaluations, the MLR is superior to the rest. My regression tree performed worst. I’ll fit the MLR finally to the entire dataset.\n\nworkflow_final_mlr |&gt; fit(data) |&gt; tidy()\n\n# A tibble: 30 × 5\n   term                            estimate std.error statistic  p.value\n   &lt;chr&gt;                              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)                       10188.     2302.     4.42  1.32e- 5\n 2 isSpringSeason                     6830.     1884.     3.63  3.34e- 4\n 3 isSummerSeason                    26625.     1814.    14.7   9.78e-38\n 4 isAutumnSeason                    11356.     1933.     5.87  1.06e- 8\n 5 isHoliday                         -2556.     1074.    -2.38  1.79e- 2\n 6 isWeekend                         -2387.      334.    -7.15  5.74e-12\n 7 isSpringSeason_x_isHoliday         -375.     2037.    -0.184 8.54e- 1\n 8 isSummerSeason_x_isHoliday        -1093.     2255.    -0.484 6.28e- 1\n 9 isAutumnSeason_x_isHoliday        -3000.     1789.    -1.68  9.45e- 2\n10 isSpringSeason_x_TemperatureAvg   10235.     2134.     4.80  2.47e- 6\n# ℹ 20 more rows"
  },
  {
    "objectID": "homework9.html#previous-work-in-homework-8",
    "href": "homework9.html#previous-work-in-homework-8",
    "title": "Homework 9",
    "section": "",
    "text": "This document demonstrates use of the principles and steps to make models in R:\n\nread data\ncheck the data\nsplit the data\nfit models\napply best model\n\n\n\n\n\nThis work relies heavily on tidymodels packages and related items, so we include this and the standard tidyverse code.\n\n\nWarning: package 'tidymodels' was built under R version 4.4.2\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n\n\n✔ broom        1.0.6     ✔ recipes      1.1.0\n✔ dials        1.3.0     ✔ rsample      1.2.1\n✔ dplyr        1.1.4     ✔ tibble       3.2.1\n✔ ggplot2      3.5.1     ✔ tidyr        1.3.1\n✔ infer        1.0.7     ✔ tune         1.2.1\n✔ modeldata    1.4.0     ✔ workflows    1.1.4\n✔ parsnip      1.2.1     ✔ workflowsets 1.1.0\n✔ purrr        1.0.2     ✔ yardstick    1.3.1\n\n\nWarning: package 'dials' was built under R version 4.4.2\n\n\nWarning: package 'infer' was built under R version 4.4.2\n\n\nWarning: package 'modeldata' was built under R version 4.4.2\n\n\nWarning: package 'parsnip' was built under R version 4.4.2\n\n\nWarning: package 'recipes' was built under R version 4.4.2\n\n\nWarning: package 'rsample' was built under R version 4.4.2\n\n\nWarning: package 'tune' was built under R version 4.4.2\n\n\nWarning: package 'workflows' was built under R version 4.4.2\n\n\nWarning: package 'workflowsets' was built under R version 4.4.2\n\n\nWarning: package 'yardstick' was built under R version 4.4.2\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Learn how to get started at https://www.tidymodels.org/start/\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ lubridate 1.9.3     ✔ stringr   1.5.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::col_factor() masks scales::col_factor()\n✖ purrr::discard()    masks scales::discard()\n✖ dplyr::filter()     masks stats::filter()\n✖ stringr::fixed()    masks recipes::fixed()\n✖ dplyr::lag()        masks stats::lag()\n✖ readr::spec()       masks yardstick::spec()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nWarning: package 'corrr' was built under R version 4.4.2\n\n\nWarning: package 'glmnet' was built under R version 4.4.2\n\n\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nLoaded glmnet 4.1-8\n\n\nWarning: package 'rpart.plot' was built under R version 4.4.2\n\n\nLoading required package: rpart\n\nAttaching package: 'rpart'\n\nThe following object is masked from 'package:dials':\n\n    prune\n\n\nWarning: package 'baguette' was built under R version 4.4.2\n\n\nWarning: package 'ranger' was built under R version 4.4.2\n\n\nWarning: package 'randomForest' was built under R version 4.4.2\n\n\nrandomForest 4.7-1.2\nType rfNews() to see new features/changes/bug fixes.\n\nAttaching package: 'randomForest'\n\nThe following object is masked from 'package:ranger':\n\n    importance\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\n\n\n\nThe data comes from the UCI Machine Learning Repository. This set is about bike sharing rentals. More details available here. The data description describes the following variables:\n\n\n\n\n\n\n\nFIELD\nNOTES\n\n\n\n\nDate\nday/month/year\n\n\nRented Bike count\nCount of bikes rented at each hour\n\n\nHour\nHour of the day\n\n\nTemperature\nTemperature in Celsius\n\n\nHumidity\n%\n\n\nWindspeed\nm/s\n\n\nVisibility\n10m\n\n\nDew point temperature\nCelsius\n\n\nSolar radiation\nMJ/m2\n\n\nRainfall\nmm\n\n\nSnowfall\ncm\n\n\nSeasons\nWinter, Spring, Summer, Autumn\n\n\nHoliday\nHoliday/No holiday\n\n\nFunctional Day\nNoFunc(Non Functional Hours), Fun(Functional hours)\n\n\n\n\n\n\n\n\ndata_url &lt;- \"https://www4.stat.ncsu.edu/~online/datasets/SeoulBikeData.csv\"\ndata_raw &lt;- read_csv(\n  file = data_url, \n  locale = locale(encoding = \"latin1\")\n  )\n\nRows: 8760 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): Date, Seasons, Holiday, Functioning Day\ndbl (10): Rented Bike Count, Hour, Temperature(°C), Humidity(%), Wind speed ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n\nNow, I need to review the data and clean it up, then summarize it.\n\n\n\ndata_raw |&gt; \n  map( ~sum(is.na(.)) )\n\n$Date\n[1] 0\n\n$`Rented Bike Count`\n[1] 0\n\n$Hour\n[1] 0\n\n$`Temperature(°C)`\n[1] 0\n\n$`Humidity(%)`\n[1] 0\n\n$`Wind speed (m/s)`\n[1] 0\n\n$`Visibility (10m)`\n[1] 0\n\n$`Dew point temperature(°C)`\n[1] 0\n\n$`Solar Radiation (MJ/m2)`\n[1] 0\n\n$`Rainfall(mm)`\n[1] 0\n\n$`Snowfall (cm)`\n[1] 0\n\n$Seasons\n[1] 0\n\n$Holiday\n[1] 0\n\n$`Functioning Day`\n[1] 0\n\n\nLooks ok, no missing values (NA).\n\n\n\nDo the column types look accurate?\n\nstr(data_raw)\n\nspc_tbl_ [8,760 × 14] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Date                     : chr [1:8760] \"01/12/2017\" \"01/12/2017\" \"01/12/2017\" \"01/12/2017\" ...\n $ Rented Bike Count        : num [1:8760] 254 204 173 107 78 100 181 460 930 490 ...\n $ Hour                     : num [1:8760] 0 1 2 3 4 5 6 7 8 9 ...\n $ Temperature(°C)          : num [1:8760] -5.2 -5.5 -6 -6.2 -6 -6.4 -6.6 -7.4 -7.6 -6.5 ...\n $ Humidity(%)              : num [1:8760] 37 38 39 40 36 37 35 38 37 27 ...\n $ Wind speed (m/s)         : num [1:8760] 2.2 0.8 1 0.9 2.3 1.5 1.3 0.9 1.1 0.5 ...\n $ Visibility (10m)         : num [1:8760] 2000 2000 2000 2000 2000 ...\n $ Dew point temperature(°C): num [1:8760] -17.6 -17.6 -17.7 -17.6 -18.6 -18.7 -19.5 -19.3 -19.8 -22.4 ...\n $ Solar Radiation (MJ/m2)  : num [1:8760] 0 0 0 0 0 0 0 0 0.01 0.23 ...\n $ Rainfall(mm)             : num [1:8760] 0 0 0 0 0 0 0 0 0 0 ...\n $ Snowfall (cm)            : num [1:8760] 0 0 0 0 0 0 0 0 0 0 ...\n $ Seasons                  : chr [1:8760] \"Winter\" \"Winter\" \"Winter\" \"Winter\" ...\n $ Holiday                  : chr [1:8760] \"No Holiday\" \"No Holiday\" \"No Holiday\" \"No Holiday\" ...\n $ Functioning Day          : chr [1:8760] \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Date = col_character(),\n  ..   `Rented Bike Count` = col_double(),\n  ..   Hour = col_double(),\n  ..   `Temperature(°C)` = col_double(),\n  ..   `Humidity(%)` = col_double(),\n  ..   `Wind speed (m/s)` = col_double(),\n  ..   `Visibility (10m)` = col_double(),\n  ..   `Dew point temperature(°C)` = col_double(),\n  ..   `Solar Radiation (MJ/m2)` = col_double(),\n  ..   `Rainfall(mm)` = col_double(),\n  ..   `Snowfall (cm)` = col_double(),\n  ..   Seasons = col_character(),\n  ..   Holiday = col_character(),\n  ..   `Functioning Day` = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nObservations and updates required:\n\nSwitch type to Date:\n\nDate\n\nSwitch type to Integer:\n\nRented Bike Count\nHour\n\nSwitch character lists to Factor:\n\nSeasons\nHoliday\nFunctioning Day\n\nRename to remove spaces across most fields\n\nDo the numerical summaries look reasonable?\n\ndata_raw |&gt;\n  select(where(is.numeric)) |&gt;\n  psych::describe() |&gt;\n  select(\n    min,\n    max,\n    range,\n    median,\n    sd\n    )\n\n                            min     max   range  median     sd\nRented Bike Count           0.0 3556.00 3556.00  504.50 645.00\nHour                        0.0   23.00   23.00   11.50   6.92\nTemperature(°C)           -17.8   39.40   57.20   13.70  11.94\nHumidity(%)                 0.0   98.00   98.00   57.00  20.36\nWind speed (m/s)            0.0    7.40    7.40    1.50   1.04\nVisibility (10m)           27.0 2000.00 1973.00 1698.00 608.30\nDew point temperature(°C) -30.6   27.20   57.80    5.10  13.06\nSolar Radiation (MJ/m2)     0.0    3.52    3.52    0.01   0.87\nRainfall(mm)                0.0   35.00   35.00    0.00   1.13\nSnowfall (cm)               0.0    8.80    8.80    0.00   0.44\n\n\nNothing looks unreasonable in the numeric variable spread.\nDo the categorical variable values look reasonable?\n\ndata_raw |&gt;\n  select(where(is_character),-Date) |&gt;\n  map(unique)\n\n$Seasons\n[1] \"Winter\" \"Spring\" \"Summer\" \"Autumn\"\n\n$Holiday\n[1] \"No Holiday\" \"Holiday\"   \n\n$`Functioning Day`\n[1] \"Yes\" \"No\" \n\n\nUnique categorical values look fine as well.\n\n\n\nNow, let’s fix the Date field format.\n\ndata_raw &lt;- data_raw |&gt;\n  mutate(Date = as_date(Date,format=\"%d/%m/%Y\"))\nstr(data_raw$Date)\n\n Date[1:8760], format: \"2017-12-01\" \"2017-12-01\" \"2017-12-01\" \"2017-12-01\" \"2017-12-01\" ...\n\n\n\n\n\nNext, turn character fields into factors.\n\ndata_raw &lt;- data_raw |&gt;\n  mutate(\n    Seasons = as_factor(Seasons),\n    Holiday = as_factor(Holiday),\n    `Functioning Day` = as_factor(`Functioning Day`)\n    )\nstr(select(data_raw,where(is.factor)))\n\ntibble [8,760 × 3] (S3: tbl_df/tbl/data.frame)\n $ Seasons        : Factor w/ 4 levels \"Winter\",\"Spring\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ Holiday        : Factor w/ 2 levels \"No Holiday\",\"Holiday\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Functioning Day: Factor w/ 2 levels \"Yes\",\"No\": 1 1 1 1 1 1 1 1 1 1 ...\n\n\nAlso, here, I will change the previously-noted fields into integers.\n\ndata_raw &lt;- data_raw |&gt;\n  mutate(\n    `Rented Bike Count` = as.integer(`Rented Bike Count`),\n    Hour = as.integer(Hour)\n    )\nstr(select(data_raw,where(is_integer)))\n\ntibble [8,760 × 5] (S3: tbl_df/tbl/data.frame)\n $ Rented Bike Count: int [1:8760] 254 204 173 107 78 100 181 460 930 490 ...\n $ Hour             : int [1:8760] 0 1 2 3 4 5 6 7 8 9 ...\n $ Seasons          : Factor w/ 4 levels \"Winter\",\"Spring\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ Holiday          : Factor w/ 2 levels \"No Holiday\",\"Holiday\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Functioning Day  : Factor w/ 2 levels \"Yes\",\"No\": 1 1 1 1 1 1 1 1 1 1 ...\n\n\n\n\n\nAnd lastly, cleaning up the names for easier work without encoding.\n\ndata_raw &lt;- data_raw |&gt;\n  mutate(\n    BikeCount = `Rented Bike Count`,\n    Temperature = `Temperature(°C)`,\n    Humidity = `Humidity(%)`,\n    WindSpeed = `Wind speed (m/s)`,\n    Visibility = `Visibility (10m)`,\n    DewPoint = `Dew point temperature(°C)`,\n    SolarRadiation = `Solar Radiation (MJ/m2)`,\n    Rainfall = `Rainfall(mm)`,\n    Snowfall = `Snowfall (cm)`,\n    FunctioningDay = `Functioning Day`,         \n    .keep='unused'\n    )\nstr(data_raw)\n\ntibble [8,760 × 14] (S3: tbl_df/tbl/data.frame)\n $ Date          : Date[1:8760], format: \"2017-12-01\" \"2017-12-01\" ...\n $ Hour          : int [1:8760] 0 1 2 3 4 5 6 7 8 9 ...\n $ Seasons       : Factor w/ 4 levels \"Winter\",\"Spring\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ Holiday       : Factor w/ 2 levels \"No Holiday\",\"Holiday\": 1 1 1 1 1 1 1 1 1 1 ...\n $ BikeCount     : int [1:8760] 254 204 173 107 78 100 181 460 930 490 ...\n $ Temperature   : num [1:8760] -5.2 -5.5 -6 -6.2 -6 -6.4 -6.6 -7.4 -7.6 -6.5 ...\n $ Humidity      : num [1:8760] 37 38 39 40 36 37 35 38 37 27 ...\n $ WindSpeed     : num [1:8760] 2.2 0.8 1 0.9 2.3 1.5 1.3 0.9 1.1 0.5 ...\n $ Visibility    : num [1:8760] 2000 2000 2000 2000 2000 ...\n $ DewPoint      : num [1:8760] -17.6 -17.6 -17.7 -17.6 -18.6 -18.7 -19.5 -19.3 -19.8 -22.4 ...\n $ SolarRadiation: num [1:8760] 0 0 0 0 0 0 0 0 0.01 0.23 ...\n $ Rainfall      : num [1:8760] 0 0 0 0 0 0 0 0 0 0 ...\n $ Snowfall      : num [1:8760] 0 0 0 0 0 0 0 0 0 0 ...\n $ FunctioningDay: Factor w/ 2 levels \"Yes\",\"No\": 1 1 1 1 1 1 1 1 1 1 ...\n\n\n\n\n\nNow, to display some summaries of the tidied data. Numeric summaries and then categorical contingency tables.\n\ndata_raw |&gt;\n  select(where(is.numeric)) |&gt;\n  psych::describe() |&gt;\n  select(\n    min,\n    max,\n    range,\n    median,\n    sd\n    )\n\n                 min     max   range  median     sd\nHour             0.0   23.00   23.00   11.50   6.92\nBikeCount        0.0 3556.00 3556.00  504.50 645.00\nTemperature    -17.8   39.40   57.20   13.70  11.94\nHumidity         0.0   98.00   98.00   57.00  20.36\nWindSpeed        0.0    7.40    7.40    1.50   1.04\nVisibility      27.0 2000.00 1973.00 1698.00 608.30\nDewPoint       -30.6   27.20   57.80    5.10  13.06\nSolarRadiation   0.0    3.52    3.52    0.01   0.87\nRainfall         0.0   35.00   35.00    0.00   1.13\nSnowfall         0.0    8.80    8.80    0.00   0.44\n\n\nNothing stands out here, as noted earlier. Now, to contingency tables for categorical variables.\n\ndata_raw |&gt;\n  group_by(Seasons) |&gt;\n  summarize(n())\n\n# A tibble: 4 × 2\n  Seasons `n()`\n  &lt;fct&gt;   &lt;int&gt;\n1 Winter   2160\n2 Spring   2208\n3 Summer   2208\n4 Autumn   2184\n\n\n\ndata_raw |&gt;\n  group_by(Holiday) |&gt;\n  summarize(n())\n\n# A tibble: 2 × 2\n  Holiday    `n()`\n  &lt;fct&gt;      &lt;int&gt;\n1 No Holiday  8328\n2 Holiday      432\n\n\n\ndata_raw |&gt;\n  group_by(FunctioningDay) |&gt;\n  summarize(n())\n\n# A tibble: 2 × 2\n  FunctioningDay `n()`\n  &lt;fct&gt;          &lt;int&gt;\n1 Yes             8465\n2 No               295\n\n\n\ndata_raw |&gt;\n  group_by(FunctioningDay,Seasons) |&gt;\n  summarize(n())\n\n`summarise()` has grouped output by 'FunctioningDay'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 6 × 3\n# Groups:   FunctioningDay [2]\n  FunctioningDay Seasons `n()`\n  &lt;fct&gt;          &lt;fct&gt;   &lt;int&gt;\n1 Yes            Winter   2160\n2 Yes            Spring   2160\n3 Yes            Summer   2208\n4 Yes            Autumn   1937\n5 No             Spring     48\n6 No             Autumn    247\n\n\nI don’t understand truly what the FunctioningDay field means. The notes say it is a target / response variable, but exactly how to interpret that is unclear to me. I’ll check grouping by this field.\n\ndata_raw |&gt;\n  group_by(FunctioningDay) |&gt;\n  summarize(\n    Min=min(BikeCount),\n    Max=max(BikeCount),\n    Avg=mean(BikeCount)\n    )\n\n# A tibble: 2 × 4\n  FunctioningDay   Min   Max   Avg\n  &lt;fct&gt;          &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n1 Yes                2  3556  729.\n2 No                 0     0    0 \n\n\nOh, it is simply an indicator of when bikes were available. I presume we do not want to study the days when bikes did not allow usage, so now we will subset to remove those days (FunctioningDay = No).\n\ndata_raw &lt;- data_raw |&gt;\n  filter(FunctioningDay == 'Yes')\n\n\n\n\nNow for simplicity, we adjust our data to summarize across hours so that each day has only one observation associated with it.\n\ndata &lt;- data_raw |&gt;\n  group_by(Date,\n           Seasons,\n           Holiday\n           ) |&gt;\n  summarize(\n    BikeCountSum = sum(BikeCount),\n    RainfallSum = sum(Rainfall),\n    SnowfallSum = sum(Snowfall),\n    TemperatureAvg = mean(Temperature),\n    HumidityAvg = mean(Humidity),\n    WindSpeedAvg = mean(WindSpeed),\n    VisibilityAvg = mean(Visibility),\n    DewPointAvg = mean(DewPoint),\n    SolarRadiationAvg = mean(SolarRadiation)\n    ) |&gt;\n  select(\n    Date,\n    Seasons,\n    Holiday,\n    ends_with(\"Sum\"),\n    ends_with(\"Avg\")\n  )\n\n`summarise()` has grouped output by 'Date', 'Seasons'. You can override using\nthe `.groups` argument.\n\nhead(data)\n\n# A tibble: 6 × 12\n# Groups:   Date, Seasons [6]\n  Date       Seasons Holiday BikeCountSum RainfallSum SnowfallSum TemperatureAvg\n  &lt;date&gt;     &lt;fct&gt;   &lt;fct&gt;          &lt;int&gt;       &lt;dbl&gt;       &lt;dbl&gt;          &lt;dbl&gt;\n1 2017-12-01 Winter  No Hol…         9539         0           0          -2.45  \n2 2017-12-02 Winter  No Hol…         8523         0           0           1.32  \n3 2017-12-03 Winter  No Hol…         7222         4           0           4.88  \n4 2017-12-04 Winter  No Hol…         8729         0.1         0          -0.304 \n5 2017-12-05 Winter  No Hol…         8307         0           0          -4.46  \n6 2017-12-06 Winter  No Hol…         6669         1.3         8.6         0.0458\n# ℹ 5 more variables: HumidityAvg &lt;dbl&gt;, WindSpeedAvg &lt;dbl&gt;,\n#   VisibilityAvg &lt;dbl&gt;, DewPointAvg &lt;dbl&gt;, SolarRadiationAvg &lt;dbl&gt;\n\n\n\n\n\nNow, to restate summaries of the updated dataset.\n\ndata |&gt;\n  select(where(is.numeric)) |&gt;\n  psych::describe() |&gt;\n  select(\n    min,\n    max,\n    range,\n    median,\n    sd\n    )\n\nAdding missing grouping variables: `Date`, `Seasons`\n\n\nWarning in FUN(newX[, i], ...): no non-missing arguments to min; returning Inf\n\n\nWarning in FUN(newX[, i], ...): no non-missing arguments to max; returning -Inf\n\n\n                     min      max    range   median      sd\nDate                 Inf     -Inf     -Inf       NA      NA\nSeasons*            1.00     4.00     3.00     2.00    1.11\nBikeCountSum      977.00 36149.00 35172.00 18563.00 9937.16\nRainfallSum         0.00    95.50    95.50     0.00   11.79\nSnowfallSum         0.00    78.70    78.70     0.00    8.80\nTemperatureAvg    -14.74    33.74    48.48    13.74   11.72\nHumidityAvg        22.25    95.88    73.62    57.17   14.87\nWindSpeedAvg        0.66     4.00     3.34     1.66    0.60\nVisibilityAvg     214.29  2000.00  1785.71  1557.75  491.16\nDewPointAvg       -27.75    25.04    52.79     4.61   12.99\nSolarRadiationAvg   0.03     1.22     1.19     0.56    0.32\n\n\nLet’s visualize this information a few ways - with box and whiskers as well as scatterplots.\n\ng &lt;- data |&gt;\n  ggplot()\ng + \n  geom_boxplot(\n    aes(\n      x=Seasons,\n      y=BikeCountSum,\n      color=Holiday\n    )\n  ) + \n  labs(\n    title=\"Bike Counts per Season by Holiday\"\n    )\n\n\n\n\n\n\n\n\nOn holidays, across all seasons, fewer bikes are used. However, the variation in range of max and min bikes used is much smaller on holidays. So, as a light interpretation notwithstanding the much smaller sample size of Holiday data, we might assess that holidays do garner a tight range of activity, consistently.\n\ng + \n  geom_point(\n    aes(\n      x=TemperatureAvg,\n      y=BikeCountSum\n    )\n  ) + \n  labs(\n    title=\"Bike Counts vs Temperature\"\n    ) +\n  facet_grid(~Seasons)\n\n\n\n\n\n\n\n\nThe shapes here are interesting. In Winter, no matter the temperature, few bikes are used. In the spring, where it can be a bit cool to a bit warm, the number of bikes used quickly grows. In the summer, in high temperatures consistently, if temperature raises slightly, bike rentals decrease rapidly. Autumn is comparable to Spring in shape and range.\nLastly, we display correlations for all numeric variables.\n\ndata |&gt;\n  select(where(is.numeric)) |&gt;\n  correlate() |&gt;\n  shave() |&gt;\n  rplot()\n\nAdding missing grouping variables: `Date`, `Seasons`\nNon-numeric variables removed from input: `Date`, and `Seasons`\nCorrelation computed with • Method: 'pearson' • Missing treated using:\n'pairwise.complete.obs'\n\n\n\n\n\n\n\n\n\nThis package corrr has cool features, including this color-coded display of all correlations between numeric variables. Immediately, we can see the strongest relationships with Bike Counts are the Temperature, Dew Point, and Solar Radiation. It’s likely those are interrelated and tell the same story (evidenced by the strong correlation between Temperature and Dew Point shown in the chart, elsewhere). The strongest negative correlation between non-result variables is that of Humidity and Visibility. I don’t normally think of humidity impacting visibility, so that’s interesting; is it because of pollution or am I simply unaware that wet air does impede visibility, perhaps at longer distances?\n\n\n\n\nTo analyze this data, which is small, we will split into training and test and then use 10-fold CV. In the split, we will use the strata argument to ensure a fair sample across the seasons variable.\n\ndata_split &lt;- initial_split(data, prop = 0.75, strata = Seasons)\ndata_train &lt;- training(data_split)\ndata_test &lt;- testing(data_split)\ndata_train_10Fold &lt;- vfold_cv(data_train, 10)\n\n\n\n\n\n\nFirst recipe, ignore Date and instead work with weekday/weekend factor. Then standardize numeric variables to make comparable scales. Create dummy variables for seasons, holiday, and the day type.\n\nrecipe1 &lt;- recipe(BikeCountSum ~ ., data = data_train) |&gt;\n  \n  #Date into weekend/weekday\n  step_date(Date) |&gt;\n  step_mutate(\n    Weekday_Weekend = factor(if_else(\n      (Date_dow == \"Sat\") | (Date_dow == \"Sun\"),\n      \"Weekend\",\n      \"Weekday\")\n      )\n    ) |&gt;\n  \n  #remove excess original Date fields\n  step_rm(c(Date,\n            Date_dow,\n            Date_month,\n            Date_year)\n          ) |&gt;\n  \n  #normalize numerics\n  step_normalize(\n    all_numeric(),\n    -all_outcomes()\n    ) |&gt;\n  \n  #dummy vars for categorical items\n  step_dummy(c(Seasons,\n               Holiday,\n               Weekday_Weekend)\n             ) |&gt;\n  \n  #clean up names\n  step_rename(\n    isHoliday = Holiday_Holiday,\n    isWeekend = Weekday_Weekend_Weekend,\n    isSummerSeason = Seasons_Summer,\n    isSpringSeason = Seasons_Spring,\n    isAutumnSeason = Seasons_Autumn\n  )\n    \n    \n   # ) |&gt;  prep(training=data_train) |&gt;\n #bake(data_train)\n#testing |&gt; summary()\n\n\n\n\nFor this recipe, we start with Recipe 1 and add interaction terms between:\n\nseasons and holiday\nseasons and temp\ntemp and rainfall\n\n\nrecipe2 &lt;- recipe1 |&gt;\n  step_interact(terms = ~\n                  ends_with(\"Season\") *\n                  ends_with(\"Holiday\") \n                ) |&gt;\n  step_interact(terms = ~\n                  ends_with(\"Season\") *\n                  TemperatureAvg\n                ) |&gt;\n  step_interact(terms = ~\n                  TemperatureAvg *\n                  RainfallSum\n                ) \n\n\n\n\nFor the third recipe, start from Recipe 2 and add quadratic terms for each numeric predictor. Since our dummy variables are technically numeric now, I’m excluding them by avoiding all those beginning with is (like isSpring, etc.).\n\nrecipe3 &lt;- recipe2 |&gt;\n  step_poly(\n    all_numeric_predictors(),\n    -starts_with(\"is\"),\n    degree=2\n    )\n\n\n\n\nWe will fit the models using linear lm engine and use 10-fold CV to calculate error.\nFirst, define the model engine.\n\ndata_model &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\n\nNext, define workflows for each recipe.\n\ndata_workflow1 &lt;- workflow() |&gt;\n  add_recipe(recipe1) |&gt;\n  add_model(data_model)\n\ndata_workflow2 &lt;- workflow() |&gt;\n  add_recipe(recipe2) |&gt;\n  add_model(data_model)\n\ndata_workflow3 &lt;- workflow() |&gt;\n  add_recipe(recipe3) |&gt;\n  add_model(data_model)\n\nNow, define and run the 10-fold CV for each. Out of curiosity, I am going to compare to a non-CV run as well.\n\n#non-CV for simple recipe 1\ndata_fit_nonCV &lt;- data_workflow1 |&gt;\n  fit(data_train)\n\n#data_fit_nonCV |&gt;\n# tidy()\n\n#10fold CV for each recipe\nrecipe1_10Fold_metrics &lt;- data_workflow1 |&gt;\n  fit_resamples(data_train_10Fold) |&gt;\n  collect_metrics()\n\nrecipe2_10Fold_metrics &lt;- data_workflow2 |&gt;\n  fit_resamples(data_train_10Fold) |&gt;\n  collect_metrics()\n\n→ A | warning: prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x2\nThere were issues with some computations   A: x2\n\n\n\n\nrecipe3_10Fold_metrics &lt;- data_workflow3 |&gt;\n  fit_resamples(data_train_10Fold) |&gt;\n  collect_metrics()\n\n→ A | warning: prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\n\n\nThere were issues with some computations   A: x2\nThere were issues with some computations   A: x2\n\n\n\n\nrbind(\n  recipe1_10Fold_metrics,\n  recipe2_10Fold_metrics,\n  recipe3_10Fold_metrics\n)\n\n# A tibble: 6 × 6\n  .metric .estimator     mean     n  std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   4068.       10 199.     Preprocessor1_Model1\n2 rsq     standard      0.821    10   0.0231 Preprocessor1_Model1\n3 rmse    standard   3133.       10 227.     Preprocessor1_Model1\n4 rsq     standard      0.891    10   0.0176 Preprocessor1_Model1\n5 rmse    standard   2899.       10 232.     Preprocessor1_Model1\n6 rsq     standard      0.906    10   0.0176 Preprocessor1_Model1\n\n\nThe best model of the three looks like the third recipe, with interaction terms and quadratic terms.\n\n\n\n\nNow, let’s fit it to the entire training dataset and compute RMSE.\n\nbest_fit &lt;- data_workflow3 |&gt;\n  last_fit(data_split)\nbest_fit |&gt; collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    3569.    Preprocessor1_Model1\n2 rsq     standard       0.895 Preprocessor1_Model1\n\n\nHere is the coefficient table for our model, arranged by p-values to highlight the most predictive parameters.\n\nextract_fit_parsnip(best_fit) |&gt; tidy() |&gt; arrange(p.value)\n\n# A tibble: 30 × 5\n   term                                estimate std.error statistic  p.value\n   &lt;chr&gt;                                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 isSummerSeason                        26491.     2012.     13.2  5.75e-30\n 2 SolarRadiationAvg_poly_1              49177.     5662.      8.68 6.74e-16\n 3 isAutumnSeason                        12903.     2114.      6.10 4.29e- 9\n 4 isWeekend                             -2308.      398.     -5.80 2.18e- 8\n 5 isSpringSeason_x_TemperatureAvg       11376.     2363.      4.82 2.65e- 6\n 6 RainfallSum_poly_1                   -51088.    12393.     -4.12 5.21e- 5\n 7 isSpringSeason                         7863.     2069.      3.80 1.84e- 4\n 8 (Intercept)                            8535.     2522.      3.38 8.37e- 4\n 9 TemperatureAvg_x_RainfallSum_poly_2   28985.     9360.      3.10 2.20e- 3\n10 isAutumnSeason_x_TemperatureAvg        7050.     2665.      2.65 8.71e- 3\n# ℹ 20 more rows\n\n\nSo, recalling what we are doing here - predicting bike rental volume - it is interesting to note the predictors most likely to relate to bike rental volumes. I think that’s what the lowest p-values represent here, the likelihood that this was a random relationship (slope of zero) with the outcome.\n\nif we are in summer, we are likely to see more rentals\nsolar radiation increases with rentals, too (related to summer)\non the weekend, we are less likely to see rentals? That surprises me, so I checked my setup to be sure.\nif raining, less bikes; this makes sense."
  },
  {
    "objectID": "homework9.html#homework-9-start",
    "href": "homework9.html#homework-9-start",
    "title": "Homework 9",
    "section": "",
    "text": "Now, we will proceed to create the following models. I can reuse data objects and recipes from the prior effort (homework 8)!\n\na (tuned) LASSO model\na (tuned) Regression Tree model\na (tuned) Bagged Tree model\na (tuned) Random Forest model\n\nI will fit and tune each on the training set, taking the best from each family, fitting on the entire training set, and then comparing the loss functions of each family on the test set. Metrics will be RMSE and MAE. Will also display some data on each of the model types and finally fit the winner to the full dataset.\n\n\nWith my MLR recipes above, the best performing result included interaction terms and quadratic terms - presuming I did that correctly, of course. Since I have no other reason to choose or exclude terms, I’ll work from that recipe again, in the LASSO framework.\nAlso, note that I’m renaming things for some clarity with competing models. My MLR models are labeled as workflow_final_mlr for the best of the MLR family of models and fit_mlr for the version of this that has trained on the entire training dataset.\n\n#renaming my mlr workflow and model and fit\nworkflow_final_mlr &lt;- data_workflow3\nmodel_mlr &lt;- data_model\nfit_mlr &lt;- workflow_final_mlr |&gt; fit(data_train)\nrecipe_mlr &lt;- recipe3\n\n#setting lasso model and workflow\nmodel_lasso &lt;- linear_reg(penalty = tune(), mixture = 1) |&gt;\n  set_engine(\"glmnet\")\n\nworkflow_lasso &lt;- workflow() |&gt;\n  add_recipe(recipe_mlr) |&gt;\n  add_model(model_lasso)\n\nNow I configure the grid for tuning to find the optimal alpha value. I don’t know what alphas to try so I’m using grid_regular().\n\n#A warning will occur for one value of the tuning parameter, safe to ignore\ngrid_lasso &lt;- workflow_lasso |&gt;\n  tune_grid(\n    resamples = data_train_10Fold,\n    grid = grid_regular(penalty(), levels = 200)*25\n    ) \n\ngrid_lasso |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\")\n\n# A tibble: 200 × 7\n         penalty .metric .estimator  mean     n std_err .config               \n           &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n 1 0.0000000025  rmse    standard   2885.    10    229. Preprocessor1_Model001\n 2 0.00000000281 rmse    standard   2885.    10    229. Preprocessor1_Model002\n 3 0.00000000315 rmse    standard   2885.    10    229. Preprocessor1_Model003\n 4 0.00000000354 rmse    standard   2885.    10    229. Preprocessor1_Model004\n 5 0.00000000397 rmse    standard   2885.    10    229. Preprocessor1_Model005\n 6 0.00000000446 rmse    standard   2885.    10    229. Preprocessor1_Model006\n 7 0.00000000501 rmse    standard   2885.    10    229. Preprocessor1_Model007\n 8 0.00000000562 rmse    standard   2885.    10    229. Preprocessor1_Model008\n 9 0.00000000631 rmse    standard   2885.    10    229. Preprocessor1_Model009\n10 0.00000000708 rmse    standard   2885.    10    229. Preprocessor1_Model010\n# ℹ 190 more rows\n\n\nI had to play with the grid here. Penalties (alphas) between 0 and 1 did not differ. Once I scaled them up a bit, I found some improvements around alpha=10. This is clearly visible with a plot.\n\ngrid_lasso |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  ggplot(aes(penalty, mean, color = .metric)) +\n  geom_line()\n\n\n\n\n\n\n\n\nNow, choosing the optimal alpha is easy with tidymodels.\n\ntune_lasso &lt;- grid_lasso |&gt;\n  select_best(metric = \"rmse\")\ntune_lasso\n\n# A tibble: 1 × 2\n  penalty .config               \n    &lt;dbl&gt; &lt;chr&gt;                 \n1    19.8 Preprocessor1_Model198\n\n\nI think the variations across runs are pretty interesting. Based on sample variants each time, I see the ideal penalty land somewhere between 5 and 20, according to the curves generated for each sample. It makes me think I have errors but then I recall and realize that it is normal variance. I suspect too that the model is only lightly influenced by penalty variations in this range, so a model with alpha of 5 is generating a similar prediction to the same with alpha of 20.\nFinishing this workflow, now training with the best alpha on the full training set, to complete the LASSO effort. I’m storing the model in workflow_final_lasso and the fit model in fit_lasso.\nThe coefficient tables for this model:\n\nworkflow_final_lasso &lt;- workflow_lasso |&gt;\n  finalize_workflow(tune_lasso)\nfit_lasso &lt;- workflow_final_lasso |&gt;\n  fit(data_train)\ntidy(fit_lasso)\n\n# A tibble: 30 × 3\n   term                            estimate penalty\n   &lt;chr&gt;                              &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)                       10039.    19.8\n 2 isSpringSeason                     6501.    19.8\n 3 isSummerSeason                    24670.    19.8\n 4 isAutumnSeason                    11475.    19.8\n 5 isHoliday                         -2830.    19.8\n 6 isWeekend                         -2352.    19.8\n 7 isSpringSeason_x_isHoliday        -3498.    19.8\n 8 isSummerSeason_x_isHoliday        -1283.    19.8\n 9 isAutumnSeason_x_isHoliday        -4763.    19.8\n10 isSpringSeason_x_TemperatureAvg   10371.    19.8\n# ℹ 20 more rows\n\n\n\n\n\nNext, a tuned regression tree model. I’m using the original recipe, less the interaction terms that are irrelevant for this model family. As I barely understand the model details, I’m going to tune all 3 parameters.\n\nrecipe_regtree &lt;- recipe_mlr |&gt;\n  step_rm(contains(\"_x_\"))\n\nmodel_regtree &lt;- \n  decision_tree(\n    tree_depth = tune(),\n    #min_n = 20,\n    min_n = tune(),\n    cost_complexity = tune()\n    ) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"regression\")\n\nworkflow_regtree &lt;- workflow() |&gt;\n  add_recipe(recipe_regtree) |&gt;\n  add_model(model_regtree)\n\nNext, setting up the tuning grid and using CV to find the options and metrics. I’m first trying to let it pick the grid itself via the dials package.\n\ntemp &lt;- workflow_regtree |&gt; \n  tune_grid(\n    resamples = data_train_10Fold\n    )\n\ntemp |&gt; \n  collect_metrics()\n\n# A tibble: 20 × 9\n   cost_complexity tree_depth min_n .metric .estimator     mean     n  std_err\n             &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt;\n 1        4.21e- 4          9    22 rmse    standard   3769.       10 331.    \n 2        4.21e- 4          9    22 rsq     standard      0.837    10   0.0322\n 3        3.05e- 9         14    34 rmse    standard   3952.       10 286.    \n 4        3.05e- 9         14    34 rsq     standard      0.823    10   0.0292\n 5        7.47e- 9         12     2 rmse    standard   3859.       10 228.    \n 6        7.47e- 9         12     2 rsq     standard      0.847    10   0.0185\n 7        8.82e- 5          5    28 rmse    standard   3971.       10 282.    \n 8        8.82e- 5          5    28 rsq     standard      0.823    10   0.0292\n 9        5.35e- 7          3     9 rmse    standard   4127.       10 284.    \n10        5.35e- 7          3     9 rsq     standard      0.815    10   0.0294\n11        4.39e- 2         12    31 rmse    standard   4726.       10 281.    \n12        4.39e- 2         12    31 rsq     standard      0.756    10   0.0315\n13        1.84e-10          5    12 rmse    standard   3850.       10 227.    \n14        1.84e-10          5    12 rsq     standard      0.838    10   0.0267\n15        1.73e- 5          1    14 rmse    standard   6362.       10 266.    \n16        1.73e- 5          1    14 rsq     standard      0.567    10   0.0377\n17        5.99e- 3          7    40 rmse    standard   4033.       10 305.    \n18        5.99e- 3          7    40 rsq     standard      0.818    10   0.0303\n19        1.18e- 7          8    18 rmse    standard   3561.       10 280.    \n20        1.18e- 7          8    18 rsq     standard      0.859    10   0.0286\n# ℹ 1 more variable: .config &lt;chr&gt;\n\n\nThat doesn’t look like enough variety to me, though I lack experience to qualify that point of view. Nonetheless, I’m going to try a larger grid and check whether the results improve.\n\ngrid_regtree &lt;- grid_regular(\n  cost_complexity(),\n  #tree_depth(range = c(3,8)),\n  tree_depth(),\n  min_n(),\n  levels = c(5, 5, 5))\n\ntemp &lt;- workflow_regtree |&gt; \n  tune_grid(\n    resamples = data_train_10Fold,\n    grid=grid_regtree\n    )\n\ntemp |&gt; \n  collect_metrics() |&gt;\n  filter(.metric==\"rmse\") |&gt;\n  arrange(mean)\n\n# A tibble: 125 × 9\n   cost_complexity tree_depth min_n .metric .estimator  mean     n std_err\n             &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n 1    0.000562              8    11 rmse    standard   3651.    10    258.\n 2    0.000562             11    11 rmse    standard   3659.    10    261.\n 3    0.000562             15    11 rmse    standard   3659.    10    261.\n 4    0.0000000001          8    11 rmse    standard   3681.    10    260.\n 5    0.0000000178          8    11 rmse    standard   3681.    10    260.\n 6    0.00000316            8    11 rmse    standard   3681.    10    260.\n 7    0.0000000001         11    11 rmse    standard   3689.    10    263.\n 8    0.0000000178         11    11 rmse    standard   3689.    10    263.\n 9    0.00000316           11    11 rmse    standard   3689.    10    263.\n10    0.0000000001         15    11 rmse    standard   3689.    10    263.\n# ℹ 115 more rows\n# ℹ 1 more variable: .config &lt;chr&gt;\n\n\nSo, that does tend to get me better results. And running this with only 125 instances rather than 1000 seems quick enough to be reasonable. Just saving the best tuning results in proper variables now:\n\ntune_regtree &lt;- temp |&gt; select_best(metric = \"rmse\")\nworkflow_final_regtree &lt;- \n  workflow_regtree |&gt;\n  finalize_workflow(tune_regtree)\n\nFinishing the fit on full training, with best tuning parameters. Model stored in workflow_final_regtree and model fit to training data in fit_regtree.\n\nfit_regtree &lt;- workflow_final_regtree |&gt;\n  fit(data_train)\n\nPlotting this monster, visually, if possible…\n\nfit_regtree |&gt; \n  extract_fit_engine() |&gt; \n  rpart.plot(roundint=FALSE)\n\n\n\n\n\n\n\n\nSo, that is super unreadable - as the lecture notes said, exchanging accuracy for interpretability. I could limit the levels parameters but that would be an arbitrary change only for readability, which I want to avoid. Temperature, solar radiation are key predictors, but then things vary by other measures of season/temperature. I’m surprised to see such correlated items appear; I might have guessed that this model would emphasize only one of a correlated set of variables - perhaps the strongest temperature / climate / season predictor, then a shift to something unrelated that was the next stronger predictor.\n\n\n\nMoving on to bagged tree, tuning similarly, etc. I’m reusing my recipe from Regression Tree, which again removes the Interaction terms. I’m going to tune it all again, though I wonder if I might simply reuse the tuning parameters from my Regression Tree effort. I will use a smaller grid for tuning.\nAlso, while I don’t need to use my old CV folds here and could use OOB, I don’t know how - and a quick search couldn’t clear it up. So, sticking with the samples in the lectures!\n\nmodel_bagtree &lt;- \n  bag_tree(\n    tree_depth = tune(), \n    min_n = tune(), \n    cost_complexity = tune()\n    ) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"regression\")\n\nworkflow_bagtree &lt;- workflow() |&gt;\n  add_recipe(recipe_regtree) |&gt;\n  add_model(model_bagtree)\n\ngrid_bagtree &lt;- grid_regular(\n  cost_complexity(),\n  tree_depth(),\n  min_n(),\n  levels = c(3, 3, 3)\n  )\n\ntemp &lt;- workflow_bagtree |&gt; \n  tune_grid(\n    resamples = data_train_10Fold,\n    grid=grid_bagtree,\n    metrics = metric_set(rmse,mae)\n    )\n\ntemp |&gt; \n  collect_metrics() |&gt;\n  #filter(.metric==\"rmse\") |&gt;\n  arrange(mean)\n\n# A tibble: 54 × 9\n   cost_complexity tree_depth min_n .metric .estimator  mean     n std_err\n             &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n 1    0.00000316            8     2 mae     standard   2214.    10    167.\n 2    0.00000316           15     2 mae     standard   2287.    10    162.\n 3    0.0000000001          8     2 mae     standard   2320.    10    185.\n 4    0.0000000001         15     2 mae     standard   2321.    10    151.\n 5    0.00000316            8    21 mae     standard   2588.    10    116.\n 6    0.0000000001          8    21 mae     standard   2597.    10    150.\n 7    0.00000316           15    21 mae     standard   2631.    10    128.\n 8    0.0000000001         15    21 mae     standard   2653.    10    166.\n 9    0.00000316           15    40 mae     standard   2862.    10    115.\n10    0.0000000001          8    40 mae     standard   2872.    10    146.\n# ℹ 44 more rows\n# ℹ 1 more variable: .config &lt;chr&gt;\n\ntune_bagtree &lt;- temp |&gt; select_best(metric=\"rmse\")\n\nNow, tuning collected, I set the final workflow and train on full training set.\n\nworkflow_final_bagtree &lt;- \n  workflow_bagtree |&gt;\n  finalize_workflow(tune_bagtree)\n\nfit_bagtree &lt;- \n  workflow_final_bagtree |&gt;\n  fit(data_train)\n\nTo visualize this one, I’ll plot each variable by its importance.\n\nextract_fit_engine(fit_bagtree)$imp |&gt;\n mutate(term = factor(term, levels = term)) |&gt;\n ggplot(aes(x = term, y = value)) +\n geom_bar(stat =\"identity\") +\n coord_flip()\n\n\n\n\n\n\n\n\nThe key predictors here are temperature, dew point, solar radiation, and humidity. These are correlated, especially temperature and solar radiation, and humidity with dew point (and temperature too), etc. Lowest strength predictors are holiday, weekend, visibility. Strange because I thought from my MLR earlier that some of those were stronger - or perhaps it is that they are related to bike volumes, yes, but not strong predictors. I hope that is the case, but I fear too that my MLR may have an error in a step.\n\n\n\nNext, tuning a random forest model, finalizing, and charting variables by importance as above. Same recipe as for other trees. Again, tuning everything algorithmically because I don’t have experience to choose any specific values. And I don’t know how to specify use of the out-of-bag observations, etc.\n\nmodel_randomforest &lt;- \n  rand_forest(\n    mtry = tune(),\n    trees = tune(),\n    min_n = tune()\n    ) |&gt;\n  set_engine(\"ranger\",importance=\"impurity\") |&gt;\n  set_mode(\"regression\")\n\nworkflow_randomforest &lt;- \n  workflow() |&gt;\n  add_recipe(recipe_regtree) |&gt;\n  add_model(model_randomforest)\n\ngrid_randomforest &lt;- \n  grid_regular(\n    mtry(range=c(1,length(recipe_regtree$var_info$role==\"predictor\"))-1),\n    trees(),\n    min_n(),\n    levels = c(3, 3, 3)\n    )\n\ntemp &lt;- workflow_randomforest |&gt; \n  tune_grid(\n    resamples = data_train_10Fold,\n    grid=grid_randomforest,\n    metrics = metric_set(rmse)\n    )\n\ntemp |&gt; \n  collect_metrics() |&gt;\n  #filter(.metric==\"rmse\") |&gt;\n  arrange(mean)\n\n# A tibble: 27 × 9\n    mtry trees min_n .metric .estimator  mean     n std_err .config             \n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n 1    11  2000     2 rmse    standard   2980.    10    183. Preprocessor1_Model…\n 2    11  1000     2 rmse    standard   2992.    10    183. Preprocessor1_Model…\n 3     5  1000     2 rmse    standard   3154.    10    136. Preprocessor1_Model…\n 4     5  2000     2 rmse    standard   3177.    10    149. Preprocessor1_Model…\n 5     0  2000     2 rmse    standard   3224.    10    139. Preprocessor1_Model…\n 6     0  1000     2 rmse    standard   3225.    10    139. Preprocessor1_Model…\n 7    11  1000    21 rmse    standard   3233.    10    152. Preprocessor1_Model…\n 8    11  2000    21 rmse    standard   3234.    10    151. Preprocessor1_Model…\n 9     5  1000    21 rmse    standard   3485.    10    130. Preprocessor1_Model…\n10     5  2000    21 rmse    standard   3512.    10    133. Preprocessor1_Model…\n# ℹ 17 more rows\n\ntune_randomforest &lt;- temp |&gt; select_best(metric=\"rmse\")\n\nNow, tuning collected, I set the final workflow and train on full training set. Then visualize the variables by importance.\n\nworkflow_final_randomforest &lt;- \n  workflow_randomforest |&gt;\n  finalize_workflow(tune_randomforest)\n\nfit_randomforest &lt;- \n  workflow_final_randomforest |&gt;\n  fit(data_train)\n\nTo visualize this one, I’ll plot each variable by its importance. This is a bit trickier and I need to realign the values and names a bit for the random forest model.\n\n#first pull out the variable.importance field\nimp_randomforest &lt;- data.frame(\n  row.names = NULL,\n  names=names(extract_fit_engine(fit_randomforest)$variable.importance),\n  values=extract_fit_engine(fit_randomforest)$variable.importance\n  )\n\n#sort for ggplot, which i do not understand\nimp_randomforest$names &lt;- \n  factor(\n    imp_randomforest$names, \n    levels = imp_randomforest$names[order(imp_randomforest$values,decreasing = TRUE)]\n  )\n\nimp_randomforest |&gt;\n  ggplot(\n    aes(\n      x=names,\n      y=values\n    )\n  ) +\n  geom_bar(stat=\"identity\") +\n  coord_flip()\n\n\n\n\n\n\n\n\nThis result aligns well with the bagged tree model; temperature, dew point, solar radiation are all still the top predictors, by a long margin."
  },
  {
    "objectID": "homework9.html#comparisons-and-final-fits",
    "href": "homework9.html#comparisons-and-final-fits",
    "title": "Homework 9",
    "section": "",
    "text": "Now, I’ll compare the final fits of all 5 models and their loss metrics (rmse, mae) on the test dataset. I wish it were easier to make these into a single, nice tibble; I know it is possible and not hard, but I’m low on time and going to leave it a bit ugly.\n\nworkflow_final_mlr |&gt;\n  last_fit(data_split,\n           metrics = metric_set(rmse,mae)\n           ) |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       3569. Preprocessor1_Model1\n2 mae     standard       2514. Preprocessor1_Model1\n\nworkflow_final_lasso |&gt;\n  last_fit(data_split,\n           metrics = metric_set(rmse,mae)\n           ) |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       3035. Preprocessor1_Model1\n2 mae     standard       2292. Preprocessor1_Model1\n\nworkflow_final_regtree |&gt;\n  last_fit(data_split,\n           metrics = metric_set(rmse,mae)\n           ) |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       3624. Preprocessor1_Model1\n2 mae     standard       2721. Preprocessor1_Model1\n\nworkflow_final_bagtree |&gt;\n  last_fit(data_split,\n           metrics = metric_set(rmse,mae)\n           ) |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       2714. Preprocessor1_Model1\n2 mae     standard       2160. Preprocessor1_Model1\n\nworkflow_final_randomforest |&gt;\n  last_fit(data_split,\n           metrics = metric_set(rmse,mae)\n           ) |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       3015. Preprocessor1_Model1\n2 mae     standard       2387. Preprocessor1_Model1\n\n\nBased on my evaluations, the MLR is superior to the rest. My regression tree performed worst. I’ll fit the MLR finally to the entire dataset.\n\nworkflow_final_mlr |&gt; fit(data) |&gt; tidy()\n\n# A tibble: 30 × 5\n   term                            estimate std.error statistic  p.value\n   &lt;chr&gt;                              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)                       10188.     2302.     4.42  1.32e- 5\n 2 isSpringSeason                     6830.     1884.     3.63  3.34e- 4\n 3 isSummerSeason                    26625.     1814.    14.7   9.78e-38\n 4 isAutumnSeason                    11356.     1933.     5.87  1.06e- 8\n 5 isHoliday                         -2556.     1074.    -2.38  1.79e- 2\n 6 isWeekend                         -2387.      334.    -7.15  5.74e-12\n 7 isSpringSeason_x_isHoliday         -375.     2037.    -0.184 8.54e- 1\n 8 isSummerSeason_x_isHoliday        -1093.     2255.    -0.484 6.28e- 1\n 9 isAutumnSeason_x_isHoliday        -3000.     1789.    -1.68  9.45e- 2\n10 isSpringSeason_x_TemperatureAvg   10235.     2134.     4.80  2.47e- 6\n# ℹ 20 more rows"
  }
]